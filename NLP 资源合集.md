## NLP 资源合集

### 预训练模型 [[Ref](https://mp.weixin.qq.com/s/8_UZeAUtBElaO9aY5iSo-g)]

**Cove：《Learned in Translation: Contextualized Word Vectors》**

Pytoch：https://github.com/salesforce/cove

Keras：https://github.com/rgsachin/CoVe

**ULMFit：《Universal Language Model Fine-tuning for Text Classification》**

Paper:  https://arxiv.org/abs/1801.06146

PyTorch：https://github.com/fastai/fastai/tree/ulmfit_v1

**ELMO：《Deep contextualized word representations》**

Paper:  https://arxiv.org/pdf/1802.05365.pdf

PyTorch：https://github.com/allenai/allennlp

TensorFlow：https://github.com/allenai/bilm-tf

**GPT-1：《Improving Language Understanding by Generative Pre-Training》**

PyTorch：https://github.com/huggingface/pytorch-openai-transformer-lm

TensorFlow：https://github.com/openai/finetune-transformer-lm

**Bert：《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》**

Paper: https://arxiv.org/pdf/1810.04805.pdf

PyTorch：https://github.com/huggingface/pytorch-pretrained-BERT

TensorFlow：https://github.com/google-research/bert

**Transformer-XL：《Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context》**

Paper: https://arxiv.org/abs/1901.02860

PyTorch：https://github.com/kimiyoung/transformer-xl/tree/master/tf

TensorFlow：https://github.com/kimiyoung/transformer-xl/tree/master/pytorch

**XLM：《Cross-lingual Language Model Pretraining》**

PyTorch：https://github.com/facebookresearch/XLM

**GPT-2：《Language Models are Unsupervised Multitask Learners》**

TensorFlow：https://github.com/openai/gpt-2

### Paper list

